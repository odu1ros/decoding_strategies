{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27637ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "from gpt import (\n",
    "    CharTokenizer, \n",
    "    SimpleTextDataset, \n",
    "    collate, \n",
    "    Transformer, \n",
    "    model_memory_size, \n",
    "    fix_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f823d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c01c0",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca0bcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Она пришла под утро.',\n",
       " 'Вошла осторожно, тихо, бесшумно ступая, плывя по комнате, словно призрак, привидение, а единственным звуком, выдававшим ее движение, был шорох накидки, прикасавшейся к голому телу. Однако именно этот исчезающе тихий, едва уловимый шелест разбудил ведьмака, а может, только вырвал из полусна, в котором он мерно колыхался, словно погруженный в бездонную топь, висящий между дном и поверхностью спокойного моря, среди легонько извивающихся нитей водорослей.',\n",
       " 'Он не пошевелился, даже не дрогнул. Девушка подпорхнула ближе, сбросила накидку, медленно, нерешительно оперлась коленом о край ложа. Он наблюдал за ней из-под опущенных ресниц, не выдавая себя. Девушка осторожно поднялась на постель, легла на него, обхватила бедрами. Опираясь на напряженные руки, скользнула по его лицу волосами. Волосы пахли ромашкой. Решительно и как бы нетерпеливо наклонилась, коснулась сосочком его века, щеки, губ. Он улыбнулся, медленно, осторожно, нежно взял ее руки в свои. Она выпрямилась, ускользая от его пальцев, лучистая, подсвеченная и от этого света нечеткая в туманном отблеске зари. Он пошевелился, но она решительным нажимом обеих рук остановила его и легкими, но настойчивыми движениями бедер добилась ответа.',\n",
       " 'Он ответил. Она уже не избегала его рук, откинула голову, встряхнула волосами. Ее кожа была холодной и поразительно гладкой. Глаза, которые он увидел, когда она приблизила свое лицо к его лицу, были огромными и темными, как глаза русалки.',\n",
       " 'Покачиваясь, он утонул в ромашковом море, а оно взбурлило и зашумело, потеряв покой.',\n",
       " 'Потом говорили, что человек этот пришел с севера, со стороны Канатчиковых ворот. Он шел, а навьюченную лошадь вел под уздцы. Надвигался вечер, и лавки канатчиков и шорников уже закрылись, а улочка опустела. Было тепло, но на человеке был черный плащ, накинутый на плечи. Он обращал на себя внимание.',\n",
       " 'Путник остановился перед трактиром «Старая Преисподняя», постоял немного, прислушиваясь к гулу голосов. Трактир, как всегда в это время, был полон народу.',\n",
       " 'Незнакомец не вошел в «Старую Преисподнюю», а повел лошадь дальше, вниз по улочке к другому трактиру, поменьше, который назывался «У Лиса». Здесь было пустовато – трактир пользовался не лучшей репутацией. Трактирщик поднял голову от бочки с солеными огурцами и смерил гостя взглядом. Чужак, все еще в плаще, стоял перед стойкой твердо, неподвижно и молчал.',\n",
       " '– Что подать?',\n",
       " '– Пива, – сказал незнакомец. Голос был неприятный.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/full_geralt.txt\", \"r\") as file:\n",
    "    raw_content = file.read()\n",
    "\n",
    "corpus = []\n",
    "for line in raw_content.split(\"\\n\"):\n",
    "    cleaned_line = line.strip(\"\\n\")\n",
    "    if not re.search(r\"\\w\", cleaned_line):\n",
    "        continue\n",
    "    corpus.append(cleaned_line)\n",
    "\n",
    "# names = pl.read_parquet(\"data/names.parquet\")\n",
    "# surnames = pl.read_parquet(\"data/surnames.parquet\")\n",
    "\n",
    "# def get_persons(names: pl.DataFrame, surnames: pl.DataFrame, n: int = 100) -> list[str]:\n",
    "#     persons = []\n",
    "#     for _ in range(n):\n",
    "#         sex = np.random.choice([\"m\", \"f\"]).item()\n",
    "#         name = names.filter(pl.col(\"gender\") == sex).sample(1).select(\"text\").item()\n",
    "#         surname = surnames.filter(pl.col(\"gender\") == sex).sample(1).select(\"text\").item()\n",
    "#         persons.append(f\"{name} {surname}\")\n",
    "#     return persons\n",
    "\n",
    "# corpus = get_persons(names, surnames, 10_000)\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4428520",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer().fit(corpus)\n",
    "# tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1686ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer vocab to json\n",
    "with open(\"data/tokenizer_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer.vocab, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9830c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "BATCH_SIZE = 1024\n",
    "MAX_SEQ_LEN = 200\n",
    "N_LAYERS = 6\n",
    "EMBEDDING_SIZE = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_KV_GROUPS = 2\n",
    "NUM_EXPERTS = 16\n",
    "NUM_EXPERTS_PER_TOKEN = 2\n",
    "HEAD_EMBEDDING_SIZE = EMBEDDING_SIZE // NUM_HEADS\n",
    "FCCN_HIDDEN_SIZE = EMBEDDING_SIZE * 4\n",
    "n_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddfafb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleTextDataset(\n",
    "    corpus=corpus,\n",
    "    fitted_tokenizer=tokenizer,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "# next(iter(dataloader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a888c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (_decoder): Decoder(\n",
       "    (_embeddings): Embedding(162, 128, padding_idx=0)\n",
       "    (_positional_embedding): RotaryPositionEmbedding()\n",
       "    (_layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (_mha): GroupedQueryAttention(\n",
       "          (_positional_embedding): RotaryPositionEmbedding()\n",
       "          (_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (_K): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (_V): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (_W_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (_q_norm): RMSNorm()\n",
       "          (_k_norm): RMSNorm()\n",
       "        )\n",
       "        (_fcnn): MoEFeedForward(\n",
       "          (_gate): Linear(in_features=128, out_features=16, bias=False)\n",
       "          (_fc1): ModuleList(\n",
       "            (0-15): 16 x Linear(in_features=128, out_features=512, bias=False)\n",
       "          )\n",
       "          (_fc2): ModuleList(\n",
       "            (0-15): 16 x Linear(in_features=128, out_features=512, bias=False)\n",
       "          )\n",
       "          (_fc3): ModuleList(\n",
       "            (0-15): 16 x Linear(in_features=512, out_features=128, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (_rms_norm1): RMSNorm()\n",
       "        (_rms_norm2): RMSNorm()\n",
       "        (_dropout): Dropout(p=0.15, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (_rms_norm): RMSNorm()\n",
       "  )\n",
       "  (_logits): Linear(in_features=128, out_features=162, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_layers=N_LAYERS,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_kv_groups=NUM_KV_GROUPS,\n",
    "    num_experts=NUM_EXPERTS,\n",
    "    num_experts_per_token=NUM_EXPERTS_PER_TOKEN,\n",
    "    head_embedding_size=HEAD_EMBEDDING_SIZE,\n",
    "    fcnn_hidden_size=FCCN_HIDDEN_SIZE,\n",
    "    dropout=0.15,\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=4e-3)\n",
    "loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "epoch_loss = []\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae11b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(n_epoch):\n",
    "    losses = []\n",
    "    print(f'Epoch {i + 1}')\n",
    "    for x in tqdm(dataloader):\n",
    "        curr_x = x[:, :-1]\n",
    "        next_x = x[:, 1:].clone()\n",
    "        next_x[(curr_x == 0) | (curr_x == 4)] = 0\n",
    "        \n",
    "        curr_x = curr_x.to(device)\n",
    "        next_x = next_x.to(device)\n",
    "        \n",
    "        logits = model(curr_x)\n",
    "        token_losses = loss_func(logits.transpose(1, 2), next_x.to(torch.long))\n",
    "        loss = token_losses.sum() / (token_losses > 0).sum()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    epoch_loss.append(np.mean(losses))\n",
    "    print(f'Loss: {epoch_loss[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc712bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 146.1440 MB\n",
      "bfloat16: 73.0720 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.4f} MB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.4f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/my_gpt_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoding_strategies_over_custom_gpt import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = GenerativeModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_layers=N_LAYERS,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_kv_groups=NUM_KV_GROUPS,\n",
    "    num_experts=NUM_EXPERTS,\n",
    "    num_experts_per_token=NUM_EXPERTS_PER_TOKEN,\n",
    "    head_embedding_size=HEAD_EMBEDDING_SIZE,\n",
    "    fcnn_hidden_size=FCCN_HIDDEN_SIZE,\n",
    "    dropout=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f022928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerativeModel(\n",
       "  (_decoder): Decoder(\n",
       "    (_embeddings): Embedding(75, 128, padding_idx=0)\n",
       "    (_positional_embedding): RotaryPositionEmbedding()\n",
       "    (_layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (_mha): GroupedQueryAttention(\n",
       "          (_positional_embedding): RotaryPositionEmbedding()\n",
       "          (_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (_K): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (_V): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (_W_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (_q_norm): RMSNorm()\n",
       "          (_k_norm): RMSNorm()\n",
       "        )\n",
       "        (_fcnn): MoEFeedForward(\n",
       "          (_gate): Linear(in_features=128, out_features=16, bias=False)\n",
       "          (_fc1): ModuleList(\n",
       "            (0-15): 16 x Linear(in_features=128, out_features=512, bias=False)\n",
       "          )\n",
       "          (_fc2): ModuleList(\n",
       "            (0-15): 16 x Linear(in_features=128, out_features=512, bias=False)\n",
       "          )\n",
       "          (_fc3): ModuleList(\n",
       "            (0-15): 16 x Linear(in_features=512, out_features=128, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (_rms_norm1): RMSNorm()\n",
       "        (_rms_norm2): RMSNorm()\n",
       "        (_dropout): Dropout(p=0.15, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (_rms_norm): RMSNorm()\n",
       "  )\n",
       "  (_logits): Linear(in_features=128, out_features=75, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.load_state_dict(model.state_dict())\n",
    "gen_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Check\n",
      "Input: е\n",
      "Starting Sampling decoding.\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Strategy: Greedy\n",
      "Output: ера Карачева\n"
     ]
    }
   ],
   "source": [
    "print(\"Generation Check\")\n",
    "test_prompt = list(tokenizer.vocab.values())[10]\n",
    "print(f\"Input: {test_prompt}\")\n",
    "print(\"Output:\", gen_model.generate(test_prompt, tokenizer, device=device, max_new_tokens=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818753ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
