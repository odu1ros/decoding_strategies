{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27637ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Импортируем классы из gpt.py\n",
    "from gpt import (\n",
    "    CharTokenizer, \n",
    "    SimpleTextDataset, \n",
    "    collate, \n",
    "    Transformer, \n",
    "    model_memory_size, \n",
    "    fix_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f823d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c01c0",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"full_geralt.txt\", \"r\") as file:\n",
    "#     raw_content = file.read()\n",
    "\n",
    "# corpus = []\n",
    "# for line in raw_content.split(\"\\n\"):\n",
    "#     cleaned_line = line.strip(\"\\n\")\n",
    "#     if not re.search(r\"\\w\", cleaned_line):\n",
    "#         continue\n",
    "#     corpus.append(cleaned_line)\n",
    "\n",
    "names = pl.read_parquet(\"names.parquet\")\n",
    "surnames = pl.read_parquet(\"surnames.parquet\")\n",
    "\n",
    "def get_persons(names: pl.DataFrame, surnames: pl.DataFrame, n: int = 100) -> list[str]:\n",
    "    persons = []\n",
    "    for _ in range(n):\n",
    "        sex = np.random.choice([\"m\", \"f\"]).item()\n",
    "        name = names.filter(pl.col(\"gender\") == sex).sample(1).select(\"text\").item()\n",
    "        surname = surnames.filter(pl.col(\"gender\") == sex).sample(1).select(\"text\").item()\n",
    "        persons.append(f\"{name} {surname}\")\n",
    "    return persons\n",
    "\n",
    "corpus = get_persons(names, surnames, 10_000)\n",
    "# corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4428520",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer().fit(corpus)\n",
    "# tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1686ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer vocab to json\n",
    "with open(\"tokenizer_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer.vocab, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "BATCH_SIZE = 1024\n",
    "MAX_SEQ_LEN = 200\n",
    "N_LAYERS = 6\n",
    "EMBEDDING_SIZE = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_KV_GROUPS = 2\n",
    "NUM_EXPERTS = 16\n",
    "NUM_EXPERTS_PER_TOKEN = 2\n",
    "HEAD_EMBEDDING_SIZE = EMBEDDING_SIZE // NUM_HEADS\n",
    "FCCN_HIDDEN_SIZE = EMBEDDING_SIZE * 4\n",
    "n_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfafb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleTextDataset(\n",
    "    corpus=corpus,\n",
    "    fitted_tokenizer=tokenizer,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "# next(iter(dataloader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_layers=N_LAYERS,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_kv_groups=NUM_KV_GROUPS,\n",
    "    num_experts=NUM_EXPERTS,\n",
    "    num_experts_per_token=NUM_EXPERTS_PER_TOKEN,\n",
    "    head_embedding_size=HEAD_EMBEDDING_SIZE,\n",
    "    fcnn_hidden_size=FCCN_HIDDEN_SIZE,\n",
    "    dropout=0.15,\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=4e-3)\n",
    "loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "epoch_loss = []\n",
    "device = \"cuda:1\"\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epoch):\n",
    "    losses = []\n",
    "    print(f'Epoch {i + 1}')\n",
    "    for x in tqdm(dataloader):\n",
    "        curr_x = x[:, :-1]\n",
    "        next_x = x[:, 1:].clone()\n",
    "        next_x[(curr_x == 0) | (curr_x == 4)] = 0\n",
    "        \n",
    "        curr_x = curr_x.to(device)\n",
    "        next_x = next_x.to(device)\n",
    "        \n",
    "        logits = model(curr_x)\n",
    "        token_losses = loss_func(logits.transpose(1, 2), next_x.to(torch.long))\n",
    "        loss = token_losses.sum() / (token_losses > 0).sum()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    epoch_loss.append(np.mean(losses))\n",
    "    print(f'Loss: {epoch_loss[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc712bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.4f} MB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.4f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_gpt_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoding_strategies_over_custom_gpt import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = GenerativeModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_layers=N_LAYERS,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_kv_groups=NUM_KV_GROUPS,\n",
    "    num_experts=NUM_EXPERTS,\n",
    "    num_experts_per_token=NUM_EXPERTS_PER_TOKEN,\n",
    "    head_embedding_size=HEAD_EMBEDDING_SIZE,\n",
    "    fcnn_hidden_size=FCCN_HIDDEN_SIZE,\n",
    "    dropout=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f022928",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.load_state_dict(model.state_dict())\n",
    "gen_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generation Check\")\n",
    "test_prompt = list(tokenizer.vocab.values())[10]\n",
    "print(f\"Input: {test_prompt}\")\n",
    "print(\"Output:\", gen_model.generate(test_prompt, tokenizer, device=device, max_new_tokens=20))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
